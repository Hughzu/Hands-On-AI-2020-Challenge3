{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Forecasting methods\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [10, 5]\n",
    "import os\n",
    "\n",
    "from main.utils.utils_methods import *\n",
    "from main.utils.utils import *\n",
    "import statsmodels.api as sm\n",
    "from pmdarima.arima import auto_arima\n",
    "\n",
    "import keras\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from main.module.mlp_multioutput import mlp_multioutput\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "source": [
    "We will simulate a time series from the following ARIMA process:\n",
    "$$\n",
    "y_t = 0.75 y_{t-1} - 0.25 y_{t-2} + 0.65 \\varepsilon_{t-1} + 0.35 \\varepsilon_{t-2} + \\varepsilon_t\n",
    "$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12345)\n",
    "\n",
    "# ARMA parameters\n",
    "arparams = np.array([.75, -.25])\n",
    "maparams = np.array([.65, .35])\n",
    "ar_term = np.r_[1, -arparams] # add zero-lag and negate\n",
    "ma_term = np.r_[1, maparams] # add zero-lag\n",
    "arma_process = sm.tsa.ArmaProcess(ar_term, ma_term)\n",
    "\n",
    "y = arma_process.generate_sample(1000)\n",
    "plt.plot(y)"
   ]
  },
  {
   "source": [
    "Fit an ARMA model to the simulated time series using auto_arima. Do you recover the true parameters? To obtain details about your model fit, you can use the following functions: summary(), arparams(), and maparams()."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ??\n"
   ]
  },
  {
   "source": [
    "Change the default parameters of the auto_arima function to get faster results. Eploit the fact that you know the \"true\" model (see above)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ??\n"
   ]
  },
  {
   "source": [
    "# Neural network forecast\n",
    "\n",
    "In the following, we will consider two neural network architectures for forecasting. Your task is to play with all the hyperparameters to obtain the best out-of-sample forecasts, i.e. on the test set.\n",
    "\n",
    "Some important hyperparameters include: n_simul (size of the dataset), LAG (the number of lagged values), LATENT_DIM (the number of units in the layer), BATCH_SIZE (number of samples per mini-batch), EPOCHS (the number of epochs), the optimizer and the early stop strategy.\n",
    "\n",
    "The dataset you will use is a time series simulated from a non-linear time series model. See below. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_simul = 1000\n",
    "n_burn = 100\n",
    "n = n_simul + n_burn\n",
    "noise = np.random.normal(size = n)\n",
    "\n",
    "y = np.zeros(n)\n",
    "y[0] = 0; y[1] = 0\n",
    "for t in range(2, n):\n",
    "    y[t] = 0.3 * y[t - 1] + 0.6 * y[t - 2] + (0.1 - 0.9 * y[t - 1] + 0.8 * y[t - 2]) * (1/( 1 + np.exp(- 10 * y[t - 1]) )) + noise[t]\n",
    "\n",
    "data = pd.DataFrame(y[n_burn:], columns = [\"series\"])\n",
    "plt.plot(data)"
   ]
  },
  {
   "source": [
    "Choose which loss function you want to experiment with. It is used later in the code to fit and evaluate a neural network model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function to be used to optimize the model parameters\n",
    "loss_fct = 'mse' # 'mae'\n",
    "# Accuracy measure to be used to evaluate test predictions.\n",
    "accuracy_measure = mse # mae # mape # smape"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The forecast horizon\n",
    "HORIZON = 3\n",
    "\n",
    "# The number of lagged values.\n",
    "LAG = 4\n",
    "\n",
    "# Data split\n",
    "n = len(data)\n",
    "n_train = int(0.6 * n)\n",
    "n_valid = int(0.2 * n)\n",
    "n_learn = n_train + n_valid\n",
    "\n",
    "train = data[:n_train]\n",
    "valid = data[n_train:n_learn]\n",
    "test = data[n_learn:n]\n",
    "\n",
    "# From time series to input-output data (also called time series embedding)\n",
    "train_inputs, valid_inputs, test_inputs, \\\n",
    "    X_train, y_train, X_valid, y_valid, \\\n",
    "        X_test, y_test = embed_data(train, valid, test, HORIZON, LAG, freq = None, variable = 'series')\n"
   ]
  },
  {
   "source": [
    "Read and try to understand the function *mlp_multioutput*. What kind of neural network architecture does it return?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "file_header = \"model_\" + \"mlp_multioutput\"\n",
    "verbose = 0\n",
    "\n",
    "optimizer_adam = keras.optimizers.Adam(learning_rate=0.01) \n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience= 100)\n",
    "\n",
    "LATENT_DIM = 5   # number of units in the RNN layer\n",
    "BATCH_SIZE = 32  # number of samples per mini-batch\n",
    "EPOCHS = 100      # maximum number of times the training algorithm will cycle through all samples\n",
    "loss = loss_fct\n",
    "\n",
    "best_val = ModelCheckpoint('../work/' + file_header + '_{epoch:02d}.h5', save_best_only=True, mode='min', period=1)\n",
    "#########################\n",
    "\n",
    "model_mlp_multioutput, history_mlp_multioutput = mlp_multioutput(X_train, y_train, X_valid, y_valid, \n",
    "                        LATENT_DIM = LATENT_DIM, \n",
    "                        BATCH_SIZE = BATCH_SIZE, \n",
    "                        EPOCHS = EPOCHS, \n",
    "                        LAG = LAG, \n",
    "                        HORIZON = HORIZON, \n",
    "                        loss = loss, \n",
    "                        optimizer = optimizer_adam,\n",
    "                        earlystop = earlystop, \n",
    "                        best_val = best_val,\n",
    "                        verbose=verbose)\n",
    "plot_learning_curves(history_mlp_multioutput)\n",
    "\n",
    "best_epoch = np.argmin(np.array(history_mlp_multioutput.history['val_loss']))+1\n",
    "filepath = '../work/' + file_header + '_{:02d}.h5'\n",
    "model_mlp_multioutput.load_weights(filepath.format(best_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #########################\n",
    "file_header = \"model_\" + \"mlp_recursive\"\n",
    "verbose = 0\n",
    "\n",
    "optimizer_adam = keras.optimizers.Adam(learning_rate=0.01) \n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience= 100)\n",
    "\n",
    "LATENT_DIM = 5   # number of units in the RNN layer\n",
    "BATCH_SIZE = 32  # number of samples per mini-batch\n",
    "EPOCHS = 100      # maximum number of times the training algorithm will cycle through all samples\n",
    "loss = loss_fct\n",
    "\n",
    "best_val = ModelCheckpoint('../work/' + file_header + '_{epoch:02d}.h5', save_best_only=True, mode='min', period=1)\n",
    "#########################\n",
    " \n",
    " _, _, _, X_train_onestep, y_train_onestep, X_valid_onestep, y_valid_onestep, _, _ = embed_data(train, valid, test, 1, LAG, freq = None, variable = 'series')\n",
    "model_mlp_recursive, history_mlp_recursive = mlp_multioutput(X_train_onestep, y_train_onestep, X_valid_onestep, y_valid_onestep, \n",
    "                        LATENT_DIM = LATENT_DIM, \n",
    "                        BATCH_SIZE = BATCH_SIZE, \n",
    "                        EPOCHS = EPOCHS, \n",
    "                        LAG = LAG, \n",
    "                        HORIZON = 1, \n",
    "                        loss = loss, \n",
    "                        optimizer = optimizer_adam,\n",
    "                        earlystop = earlystop, \n",
    "                        best_val = best_val,\n",
    "                        verbose=verbose)\n",
    "plot_learning_curves(history_mlp_recursive)\n",
    "\n",
    "best_epoch = np.argmin(np.array(history_mlp_recursive.history['val_loss']))+1\n",
    "filepath = '../work/' + file_header + '_{:02d}.h5'\n",
    "model_mlp_recursive.load_weights(filepath.format(best_epoch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(X_test.values[:, -1])\n",
    "predictions_naive = np.tile(X_test.values[:, -1], (HORIZON, 1) ).T\n",
    "predictions_naive = pd.DataFrame(predictions_naive, columns=['t+'+str(t) for t in range(1, HORIZON+1)])\n",
    "print(predictions_naive)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "predictions_mlp_multioutput = model_mlp_multioutput.predict(X_test)\n",
    "predictions_mlp_multioutput = pd.DataFrame(predictions_mlp_multioutput, columns=['t+'+str(t) for t in range(1, HORIZON+1)])\n",
    "\n",
    "#\n",
    "for h in range(HORIZON):\n",
    "    pred = model_mlp_recursive.predict(X_test)\n",
    "    X_test = pd.DataFrame(np.hstack( (np.delete(X_test.to_numpy(), 1, 1), pred) ), index = X_test.index, columns =X_test.columns)\n",
    "    if h > 0:\n",
    "        predictions_mlp_recursive = np.hstack( (predictions_mlp_recursive, pred) )\n",
    "    else:\n",
    "        predictions_mlp_recursive = pred\n",
    "predictions_mlp_recursive = pd.DataFrame(predictions_mlp_recursive, columns=['t+'+str(t) for t in range(1, HORIZON+1)])\n",
    "\n",
    "predictions_combination = (predictions_mlp_multioutput + predictions_mlp_recursive)/2\n",
    "\n",
    "print(predictions_mlp_multioutput)\n",
    "print(predictions_mlp_recursive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_values = pd.DataFrame(test_inputs[\"target\"], columns=['t+'+str(t) for t in range(1, HORIZON+1)])\n",
    "\n",
    "results_naive = list()\n",
    "results_mlp_multioutput = list()\n",
    "results_mlp_recursive = list()\n",
    "results_combination = list()\n",
    "\n",
    "\n",
    "for h in range(1, HORIZON+1):\n",
    "    time_horizon = 't+'+ str(h)\n",
    "    results_naive.append(accuracy_measure(true_values[time_horizon], predictions_naive[time_horizon]))\n",
    "    results_mlp_multioutput.append(accuracy_measure(true_values[time_horizon], predictions_mlp_multioutput[time_horizon]))\n",
    "    results_mlp_recursive.append(accuracy_measure(true_values[time_horizon], predictions_mlp_recursive[time_horizon]))\n",
    "    results_combination.append(accuracy_measure(true_values[time_horizon], predictions_combination[time_horizon]))\n",
    "\n",
    "\n",
    "print(np.mean(results_naive))\n",
    "print(np.mean(results_mlp_multioutput))\n",
    "print(np.mean(results_mlp_recursive))\n",
    "print(np.mean(results_combination))\n",
    "\n",
    "\n",
    "#print( mape(true_values.to_numpy().T.ravel(), predictions_mlp.to_numpy().T.ravel()) )\n",
    "\n"
   ]
  },
  {
   "source": [
    "Run the four previous forecasting methods on one time series of the Kaggle dataset, and compare their performances. Do not forget to apply transformation to make the series stationary before running neural network models."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}